{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on docs:\n",
    "# https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html\n",
    "\n",
    "# Also, see:\n",
    "# https://github.com/giakoumoglou/classification/blob/main/notebooks/main_simclr.ipynb\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Union, List, Tuple\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform, utils\n",
    "\n",
    "from utils import (\n",
    "    generate_embeddings,\n",
    "    get_image_as_np_array,\n",
    "    plot_knn_clusters,\n",
    "    get_distance_between_points_in_cluster,\n",
    "    get_distances_between_centroids,\n",
    "    plot_clusters,\n",
    "    generate_embeddings_simclr,\n",
    "    check_labels_correspondence,\n",
    "    plot_knn_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We set some configuration parameters for our experiment.\n",
    "Feel free to change them and analyze the effect.\n",
    "\n",
    "The default configuration with a batch size of 256 and input resolution of 128\n",
    "requires 6GB of GPU memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size = 32  # laptop\n",
    "# input_size = 128  # PC\n",
    "batch_size = 64  # laptop\n",
    "# batch_size = 256  # PC\n",
    "\n",
    "num_workers = 8\n",
    "seed = 1\n",
    "max_epochs = 10\n",
    "num_ftrs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the seed for our experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"./datasets/mnist\", exist_ok=True)\n",
    "os.makedirs(\"./datasets/mnist/train\", exist_ok=True)\n",
    "os.makedirs(\"./datasets/mnist/test\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset (if not already downloaded)\n",
    "train_dataset = datasets.MNIST(root=\"./datasets/mnist\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/mnist\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([10000])\n",
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])\n",
      "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))\n"
     ]
    }
   ],
   "source": [
    "print(type(test_dataset.targets))\n",
    "print(test_dataset.targets.shape)\n",
    "print(test_dataset.targets[:10])\n",
    "print(test_dataset.targets.unique(return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the images to the directory\n",
    "for i, (image, label) in enumerate(train_dataset):\n",
    "    image.save(f\"./datasets/mnist/train/{i}.png\")\n",
    "\n",
    "for i, (image, label) in enumerate(test_dataset):\n",
    "    image.save(f\"./datasets/mnist/test/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove original data to prevent errors (if it exists)\n",
    "# if os.path.exists(\"./datasets/mnist/MNIST\"):\n",
    "# shutil.rmtree(\"./datasets/mnist/MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_data = r\"./datasets/mnist\"\n",
    "path_to_train_data = Path(path_to_data) / \"train\"\n",
    "path_to_test_data = Path(path_to_data) / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data augmentations and loaders\n",
    "\n",
    "The images from the dataset have been taken from above when the clothing was\n",
    "on a table, bed or floor. Therefore, we can make use of additional augmentations\n",
    "such as vertical flip or random rotation (90 degrees).\n",
    "By adding these augmentations we learn our model invariance regarding the\n",
    "orientation of the clothing piece. E.g. we don't care if a shirt is upside down\n",
    "but more about the strcture which make it a shirt.\n",
    "\n",
    "You can learn more about the different augmentations and learned invariances\n",
    "here: `lightly-advanced`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = SimCLRTransform(input_size=input_size, vf_prob=0.5, rr_prob=0.5)\n",
    "\n",
    "# We create a torchvision transformation for embedding the dataset after\n",
    "# training\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((input_size, input_size)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=(0.5,), std=(0.5,)\n",
    "        ),  # Adjusted for single-channel images\n",
    "        # torchvision.transforms.Normalize(\n",
    "        #     mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "        #     std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        # ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./datasets/mnist\", train=False, download=True, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "7\n",
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))\n",
    "print(test_dataset[0][1])\n",
    "print(test_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train_simclr = LightlyDataset(input_dir=path_to_data, transform=transform)\n",
    "train_dataset_simclr = LightlyDataset(input_dir=path_to_train_data, transform=transform)\n",
    "\n",
    "# dataset_test = LightlyDataset(input_dir=path_to_data, transform=test_transform)\n",
    "test_dataset_simclr = LightlyDataset(\n",
    "    input_dir=path_to_test_data, transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset_simclr))\n",
    "print(len(test_dataset_simclr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    train_dataset_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "dataloader_test_simclr = torch.utils.data.DataLoader(\n",
    "    test_dataset_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general\n",
      "<class 'list'>\n",
      "3\n",
      "\n",
      "sample\n",
      "<class 'list'>\n",
      "2\n",
      "\n",
      "sample[0]\n",
      "<class 'torch.Tensor'>\n",
      "64\n",
      "\n",
      "sample[1]\n",
      "<class 'torch.Tensor'>\n",
      "64\n",
      "\n",
      "target\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64])\n",
      "\n",
      "target[0]\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n",
      "tensor(0)\n",
      "\n",
      "target[1]\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([])\n",
      "\n",
      "fname\n",
      "<class 'tuple'>\n",
      "64\n",
      "\n",
      "fname[0]\n",
      "<class 'str'>\n",
      "32401.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train_simclr:\n",
    "    # batch_size = 256\n",
    "\n",
    "    # as per docs (sample, target, fname)\n",
    "    print(\"general\")\n",
    "    print(type(batch))\n",
    "    print(len(batch))\n",
    "    print()\n",
    "\n",
    "    print(\"sample\")\n",
    "    print(type(batch[0]))\n",
    "    print(len(batch[0]))\n",
    "    print()\n",
    "\n",
    "    print(\"sample[0]\")\n",
    "    print(type(batch[0][0]))\n",
    "    print(len(batch[0][0]))\n",
    "    print()\n",
    "\n",
    "    print(\"sample[1]\")\n",
    "    print(type(batch[0][0]))\n",
    "    print(len(batch[0][0]))\n",
    "    print()\n",
    "\n",
    "    print(\"target\")\n",
    "    print(type(batch[1]))\n",
    "    print(batch[1].shape)\n",
    "    print()\n",
    "\n",
    "    print(\"target[0]\")\n",
    "    print(type(batch[1][0]))\n",
    "    print(batch[1][0].shape)\n",
    "    print(batch[1][0])\n",
    "    print()\n",
    "\n",
    "    print(\"target[1]\")\n",
    "    print(type(batch[1][1]))\n",
    "    print(batch[1][1].shape)\n",
    "    print()\n",
    "\n",
    "    print(\"fname\")\n",
    "    print(type(batch[2]))\n",
    "    print(len(batch[2]))\n",
    "    print()\n",
    "\n",
    "    print(\"fname[0]\")\n",
    "    print(type(batch[2][0]))\n",
    "    print(batch[2][0])\n",
    "    print()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SimCLR Model\n",
    "Now we create the SimCLR model. We implement it as a PyTorch Lightning Module\n",
    "and use a ResNet-18 backbone from Torchvision. Lightly provides implementations\n",
    "of the SimCLR projection head and loss function in the `SimCLRProjectionHead`\n",
    "and `NTXentLoss` classes. We can simply import them and combine the building\n",
    "blocks in the module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead\n",
    "\n",
    "\n",
    "class SimCLRModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        hidden_dim = resnet.fc.in_features\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        # TODO: dopisać inne metryki, które sprawdzają czy trening 'ma sens'\n",
    "        # czyli np. czy klasy się zbliżyły do siebie i oddaliły od innych klas\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the module using the PyTorch Lightning Trainer on a single GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment for training (may take some time)\n",
    "\n",
    "# model = SimCLRModel()\n",
    "# trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "# trainer.fit(model, dataloader_train_simclr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a helper function to generate embeddings\n",
    "from our test images using the model we just trained.\n",
    "Note that only the backbone is needed to generate embeddings,\n",
    "the projection head is only required for the training.\n",
    "Make sure to put the model into eval mode for this part!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For eval only\n",
    "\n",
    "# PC-trained model\n",
    "model = SimCLRModel.load_from_checkpoint(\n",
    "    \"./lightning_logs/version_8/checkpoints/epoch=9-step=2340.ckpt\"\n",
    ")\n",
    "\n",
    "# laptop-trained model\n",
    "# maybe not-OK, as it was trained on 70k images (both train and test)\n",
    "# model = SimCLRModel.load_from_checkpoint(\n",
    "#     \"./lightning_logs/version_0/checkpoints/epoch=9-step=10930.ckpt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimCLRModel(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (projection_head): SimCLRProjectionHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=512, out_features=128, bias=False)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): NTXentLoss(\n",
       "    (cross_entropy): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings, filenames = generate_embeddings_simclr(model, dataloader_test_simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = generate_embeddings(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters in Embedding Space\n",
    "Let's look at the trained embedding how they're clustered in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.targets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_between_points_in_cluster(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = get_distances_between_centroids(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Distances shape: {distances.shape}\")\n",
    "print(f\"Distances rank: {np.linalg.matrix_rank(distances)}\")\n",
    "print(f\"Distancess:\\n {distances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(distances, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_df = pd.DataFrame(distances)\n",
    "print(visualization_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Nearest Neighbors\n",
    "Let's look at the trained embedding and visualize the nearest neighbors for\n",
    "a few random samples.\n",
    "\n",
    "We create some helper functions to simplify the work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_examples(\n",
    "    embeddings, filenames, n_neighbors=7, num_examples=10, base_path=path_to_test_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Nearest Neighbours on 2D Plane\n",
    "Let's look at the trained embedding, perform KNN, then PCA and visualize clusters in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_clusters(\n",
    "    embeddings, np.array(test_dataset.targets), n_neighbors=5, num_examples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters on 2D Plane\n",
    "Let's look at the trained embedding, perform PCA and visualize clusters in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 'labels' created by simclr\n",
    "print(type(test_dataset_simclr.dataset.targets))\n",
    "unique_targets = np.unique(test_dataset_simclr.dataset.targets)\n",
    "print(unique_targets)\n",
    "print(test_dataset_simclr.dataset.targets[0])\n",
    "print(type(test_dataset_simclr.dataset.targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original MNIST labels\n",
    "print(type(test_dataset.targets))\n",
    "unique_targets = np.unique(test_dataset.targets)\n",
    "print(unique_targets)\n",
    "print(test_dataset.targets[0])\n",
    "print(type(test_dataset.targets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.01,\n",
    "    alpha=0.1,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=list(range(10)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.5,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[0, 1, 2, 3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.5,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[0, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.5,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[1, 4],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.5,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[2, 3, 5, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings,\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.5,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[1, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Labels Correspoding to Clusters\n",
    "Let's check if the clusters in the embedding space correspond to the labels of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_labels_correspondence(\n",
    "    embeddings=embeddings, base_path=path_to_test_data, filenames=filenames\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple clustering on MNIST\n",
    "Let's check if it's possible to create good clusters from MNIST, working in pixel space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.targets[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_clusters = KMeans(n_clusters=10).fit(test_dataset.data.reshape(-1, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mnist_clusters.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    test_dataset.data.reshape(-1, 28 * 28),\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.4,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[0, 4, 7],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    test_dataset.data.reshape(-1, 28 * 28),\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.4,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[1, 4, 7],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    test_dataset.data.reshape(-1, 28 * 28),\n",
    "    np.array(test_dataset.targets),\n",
    "    proportion_of_points_to_plot=0.4,\n",
    "    alpha=0.3,\n",
    "    plot_centroids=True,\n",
    "    specific_labels=[2, 3, 5, 8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
