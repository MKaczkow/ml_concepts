{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Straight from docs:\n",
        "# https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Tutorial 3: Train SimCLR on Clothing\n",
        "\n",
        "In this tutorial, we will train a SimCLR model using lightly. The model,\n",
        "augmentations and training procedure is from \n",
        "[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709).\n",
        "\n",
        "The paper explores a rather simple training procedure for contrastive learning.\n",
        "Since we use the typical contrastive learning loss based on NCE the method\n",
        "greatly benefits from having larger batch sizes. In this example, we use a batch\n",
        "size of 256 and paired with the input resolution per image of 64x64 pixels and\n",
        "a resnet-18 model this example requires 16GB of GPU memory.\n",
        "\n",
        "We use the \n",
        "[clothing dataset from Alex Grigorev](https://github.com/alexeygrigorev/clothing-dataset) \n",
        "for this tutorial.\n",
        "\n",
        "In this tutorial you will learn:\n",
        "\n",
        "- How to create a SimCLR model\n",
        "\n",
        "- How to generate image representations\n",
        "\n",
        "- How different augmentations impact the learned representations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "\n",
        "Import the Python frameworks we need for this tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 127] Nie można odnaleźć określonej procedury",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\__init__.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     _logger\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mStreamHandler())\n\u001b[0;32m     32\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The PyTorch Lightning team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_stats_monitor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceStatsMonitor\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\callback.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEP_OUTPUT\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCallback\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"General utilities.\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply_func\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m move_data_to_device  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AllGatherGrad  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     _AcceleratorType,\n\u001b[0;32m     22\u001b[0m     _StrategyType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     LightningEnum,\n\u001b[0;32m     27\u001b[0m )\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _compare_version, _TORCHTEXT_LEGACY\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rank_zero_deprecation\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCHTEXT_LEGACY:\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\imports.py:153\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m _TORCH_QUANTIZE_AVAILABLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m([eg \u001b[38;5;28;01mfor\u001b[39;00m eg \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39msupported_engines \u001b[38;5;28;01mif\u001b[39;00m eg \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    152\u001b[0m _TORCHTEXT_AVAILABLE \u001b[38;5;241m=\u001b[39m _package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m _TORCHTEXT_LEGACY: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m _TORCHTEXT_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m_compare_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.11.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m _TORCHVISION_AVAILABLE \u001b[38;5;241m=\u001b[39m _package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m _XLA_AVAILABLE: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m _package_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_xla\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\utilities\\imports.py:71\u001b[0m, in \u001b[0;36m_compare_version\u001b[1;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m>>> _compare_version(\"torch\", operator.ge, \"0.1\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, DistributionNotFound):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\_ops.py:933\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    928\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 127] Nie można odnaleźć określonej procedury"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.transforms import SimCLRTransform, utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "We set some configuration parameters for our experiment.\n",
        "Feel free to change them and analyze the effect.\n",
        "\n",
        "The default configuration with a batch size of 256 and input resolution of 128\n",
        "requires 6GB of GPU memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_workers = 8\n",
        "batch_size = 256\n",
        "seed = 1\n",
        "max_epochs = 20\n",
        "input_size = 128\n",
        "num_ftrs = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's set the seed for our experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make sure `path_to_data` points to the downloaded clothing dataset.\n",
        "You can download it using\n",
        "`git clone https://github.com/alexeygrigorev/clothing-dataset.git`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path_to_data = \"/datasets/clothing-dataset/images\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup data augmentations and loaders\n",
        "\n",
        "The images from the dataset have been taken from above when the clothing was\n",
        "on a table, bed or floor. Therefore, we can make use of additional augmentations\n",
        "such as vertical flip or random rotation (90 degrees).\n",
        "By adding these augmentations we learn our model invariance regarding the\n",
        "orientation of the clothing piece. E.g. we don't care if a shirt is upside down\n",
        "but more about the strcture which make it a shirt.\n",
        "\n",
        "You can learn more about the different augmentations and learned invariances\n",
        "here: `lightly-advanced`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = SimCLRTransform(input_size=input_size, vf_prob=0.5, rr_prob=0.5)\n",
        "\n",
        "# We create a torchvision transformation for embedding the dataset after\n",
        "# training\n",
        "test_transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize((input_size, input_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(\n",
        "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
        "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset_train_simclr = LightlyDataset(input_dir=path_to_data, transform=transform)\n",
        "\n",
        "dataset_test = LightlyDataset(input_dir=path_to_data, transform=test_transform)\n",
        "\n",
        "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
        "    dataset_train_simclr,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers,\n",
        ")\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the SimCLR Model\n",
        "Now we create the SimCLR model. We implement it as a PyTorch Lightning Module\n",
        "and use a ResNet-18 backbone from Torchvision. Lightly provides implementations\n",
        "of the SimCLR projection head and loss function in the `SimCLRProjectionHead`\n",
        "and `NTXentLoss` classes. We can simply import them and combine the building\n",
        "blocks in the module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from lightly.loss import NTXentLoss\n",
        "from lightly.models.modules.heads import SimCLRProjectionHead\n",
        "\n",
        "\n",
        "class SimCLRModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # create a ResNet backbone and remove the classification head\n",
        "        resnet = torchvision.models.resnet18()\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        hidden_dim = resnet.fc.in_features\n",
        "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n",
        "\n",
        "        self.criterion = NTXentLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(h)\n",
        "        return z\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        (x0, x1), _, _ = batch\n",
        "        z0 = self.forward(x0)\n",
        "        z1 = self.forward(x1)\n",
        "        loss = self.criterion(z0, z1)\n",
        "        self.log(\"train_loss_ssl\", loss)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.SGD(\n",
        "            self.parameters(), lr=6e-2, momentum=0.9, weight_decay=5e-4\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
        "        return [optim], [scheduler]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the module using the PyTorch Lightning Trainer on a single GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = SimCLRModel()\n",
        "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
        "trainer.fit(model, dataloader_train_simclr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create a helper function to generate embeddings\n",
        "from our test images using the model we just trained.\n",
        "Note that only the backbone is needed to generate embeddings,\n",
        "the projection head is only required for the training.\n",
        "Make sure to put the model into eval mode for this part!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(model, dataloader):\n",
        "    \"\"\"Generates representations for all images in the dataloader with\n",
        "    the given model\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings = []\n",
        "    filenames = []\n",
        "    with torch.no_grad():\n",
        "        for img, _, fnames in dataloader:\n",
        "            img = img.to(model.device)\n",
        "            emb = model.backbone(img).flatten(start_dim=1)\n",
        "            embeddings.append(emb)\n",
        "            filenames.extend(fnames)\n",
        "\n",
        "    embeddings = torch.cat(embeddings, 0)\n",
        "    embeddings = normalize(embeddings)\n",
        "    return embeddings, filenames\n",
        "\n",
        "\n",
        "model.eval()\n",
        "embeddings, filenames = generate_embeddings(model, dataloader_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Nearest Neighbors\n",
        "Let's look at the trained embedding and visualize the nearest neighbors for\n",
        "a few random samples.\n",
        "\n",
        "We create some helper functions to simplify the work\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_image_as_np_array(filename: str):\n",
        "    \"\"\"Returns an image as an numpy array\"\"\"\n",
        "    img = Image.open(filename)\n",
        "    return np.asarray(img)\n",
        "\n",
        "\n",
        "def plot_knn_examples(embeddings, filenames, n_neighbors=3, num_examples=6):\n",
        "    \"\"\"Plots multiple rows of random images with their nearest neighbors\"\"\"\n",
        "    # lets look at the nearest neighbors for some samples\n",
        "    # we use the sklearn library\n",
        "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(embeddings)\n",
        "    distances, indices = nbrs.kneighbors(embeddings)\n",
        "\n",
        "    # get 5 random samples\n",
        "    samples_idx = np.random.choice(len(indices), size=num_examples, replace=False)\n",
        "\n",
        "    # loop through our randomly picked samples\n",
        "    for idx in samples_idx:\n",
        "        fig = plt.figure()\n",
        "        # loop through their nearest neighbors\n",
        "        for plot_x_offset, neighbor_idx in enumerate(indices[idx]):\n",
        "            # add the subplot\n",
        "            ax = fig.add_subplot(1, len(indices[idx]), plot_x_offset + 1)\n",
        "            # get the correponding filename for the current index\n",
        "            fname = os.path.join(path_to_data, filenames[neighbor_idx])\n",
        "            # plot the image\n",
        "            plt.imshow(get_image_as_np_array(fname))\n",
        "            # set the title to the distance of the neighbor\n",
        "            ax.set_title(f\"d={distances[idx][plot_x_offset]:.3f}\")\n",
        "            # let's disable the axis\n",
        "            plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do the plot of the images. The leftmost image is the query image whereas\n",
        "the ones next to it on the same row are the nearest neighbors.\n",
        "In the title we see the distance of the neigbor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_knn_examples(embeddings, filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Color Invariance\n",
        "Let's train again without color augmentation. This will force our model to\n",
        "respect the colors in the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set color jitter and gray scale probability to 0\n",
        "new_transform = SimCLRTransform(\n",
        "    input_size=input_size, vf_prob=0.5, rr_prob=0.5, cj_prob=0.0, random_gray_scale=0.0\n",
        ")\n",
        "\n",
        "# let's update the transform on the training dataset\n",
        "dataset_train_simclr.transform = new_transform\n",
        "\n",
        "# then train a new model\n",
        "model = SimCLRModel()\n",
        "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
        "trainer.fit(model, dataloader_train_simclr)\n",
        "\n",
        "# and generate again embeddings from the test set\n",
        "model.eval()\n",
        "embeddings, filenames = generate_embeddings(model, dataloader_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "other example\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_knn_examples(embeddings, filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What's next?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# You could use the pre-trained model and train a classifier on top.\n",
        "pretrained_resnet_backbone = model.backbone\n",
        "\n",
        "# you can also store the backbone and use it in another code\n",
        "state_dict = {\"resnet18_parameters\": pretrained_resnet_backbone.state_dict()}\n",
        "torch.save(state_dict, \"model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "THIS COULD BE IN A NEW FILE (e.g. inference.py)\n",
        "\n",
        "Make sure you place the `model.pth` file in the same folder as this code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load the model in a new file for inference\n",
        "resnet18_new = torchvision.models.resnet18()\n",
        "\n",
        "# note that we need to create exactly the same backbone in order to load the weights\n",
        "backbone_new = nn.Sequential(*list(resnet18_new.children())[:-1])\n",
        "\n",
        "ckpt = torch.load(\"model.pth\")\n",
        "backbone_new.load_state_dict(ckpt[\"resnet18_parameters\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Interested in exploring other self-supervised models? Check out our other\n",
        "tutorials:\n",
        "\n",
        "- `lightly-moco-tutorial-2`\n",
        "- `lightly-simsiam-tutorial-4`\n",
        "- `lightly-custom-augmentation-5`\n",
        "- `lightly-detectron-tutorial-6`\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
