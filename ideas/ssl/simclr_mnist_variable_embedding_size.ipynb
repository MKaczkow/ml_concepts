{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on docs:\n",
    "# https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html\n",
    "\n",
    "# Also, see:\n",
    "# https://github.com/giakoumoglou/classification/blob/main/notebooks/main_simclr.ipynb\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger, MLFlowLogger\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms import SimCLRTransform, utils\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from utils import generate_embeddings, prepare_mnist_images\n",
    "from callbacks import LossLoggingCallback, HiddenDimensionsCheckingCallback\n",
    "from models import SimCLRModelVariableEmbeddingSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using accelerator: gpu\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "accelerator = \"gpu\" if device == \"cuda\" else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using accelerator: {accelerator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input_size = 32  # laptop\n",
    "input_size = 128  # PC\n",
    "# batch_size = 64  # laptop\n",
    "batch_size = 256  # PC\n",
    "\n",
    "num_workers = 8\n",
    "seed = 1\n",
    "max_epochs = 20\n",
    "num_ftrs = 32\n",
    "\n",
    "path_to_data = Path(\"datasets/MNIST\")\n",
    "path_to_train_data = path_to_data / \"train\"\n",
    "path_to_test_data = path_to_data / \"test\"\n",
    "\n",
    "experiment_name = \"simclr_mnist_variable_embedding_size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the seed for our experiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/__repos/ml_concepts/ideas/ssl/mlruns/428326800771019689', creation_time=1720109184211, experiment_id='428326800771019689', last_update_time=1720109184211, lifecycle_stage='active', name='simclr_mnist_variable_embedding_size', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Here, we define the configuration for our experiment, to be logged in mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list_of_lists = [[0, 4, 9], [0, 2, 4, 6, 9], [0, 1, 2, 4, 5, 7, 8]]\n",
    "latent_space_sizes_list = [8, 16, 32, 64, 128, 256, 512]\n",
    "projections_head_hidden_dims_list = [512]  # fixed for now\n",
    "\n",
    "# initial_num_components_pca_list = [50, 20, 7, 3]   # not used here\n",
    "initial_num_components_pca_list = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(save_dir=\"lightning_logs\", name=experiment_name)\n",
    "mlflow_logger = MLFlowLogger(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 4.1 K \n",
      "2 | projection_head | SimCLRProjectionHead | 70.9 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.006    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf54af12c9844f6b7b8644fc208d506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Nie można odnaleźć określonego pliku\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\Maciek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 501, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\Maciek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 966, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\Maciek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1435, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 8.2 K \n",
      "2 | projection_head | SimCLRProjectionHead | 75.0 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.039    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db4a91e1441459bb03b17ec5141c217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 16.4 K\n",
      "2 | projection_head | SimCLRProjectionHead | 83.2 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.105    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f9c7e8590c4d59a6f20eb687717755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 32.8 K\n",
      "2 | projection_head | SimCLRProjectionHead | 99.6 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.236    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce531d9985b647a2b07203d14ba889b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 65.7 K\n",
      "2 | projection_head | SimCLRProjectionHead | 132 K \n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 M    Total params\n",
      "45.498    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4326bd044b334129b4d48841330e187b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 131 K \n",
      "2 | projection_head | SimCLRProjectionHead | 197 K \n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.023    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d8e9c8ef9444f0bf7efe3f593a4837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 262 K \n",
      "2 | projection_head | SimCLRProjectionHead | 328 K \n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.8 M    Total params\n",
      "47.073    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a172e483f4244a6425262f25805d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 4.1 K \n",
      "2 | projection_head | SimCLRProjectionHead | 70.9 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.006    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9147dd9678ae410198990574340eba23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 8.2 K \n",
      "2 | projection_head | SimCLRProjectionHead | 75.0 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.039    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b70a94d593467ba6efdfb07cbca3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory lightning_logs\\simclr_mnist_variable_embedding_size\\version_7\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M\n",
      "1 | connector       | Linear               | 16.4 K\n",
      "2 | projection_head | SimCLRProjectionHead | 83.2 K\n",
      "3 | criterion       | NTXentLoss           | 0     \n",
      "---------------------------------------------------------\n",
      "11.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.3 M    Total params\n",
      "45.105    Total estimated model params size (MB)\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfaebfdfb5a408abaca0ff719a278ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for (\n",
    "    classes_list,\n",
    "    initial_num_components_pca,\n",
    "    latent_space_size,\n",
    "    projection_head_hidden_dim,\n",
    ") in product(\n",
    "    classes_list_of_lists,\n",
    "    initial_num_components_pca_list,\n",
    "    latent_space_sizes_list,\n",
    "    projections_head_hidden_dims_list,\n",
    "):\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"classes_list\", classes_list)\n",
    "        mlflow.log_param(\"initial_num_components_pca\", initial_num_components_pca)\n",
    "        mlflow.log_param(\"initial_latent_space_size\", latent_space_size)\n",
    "        mlflow.log_param(\"initial_projection_head_hidden_dim\", projection_head_hidden_dim)\n",
    "        mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "        # Prepare the data\n",
    "        prepare_mnist_images(classes_list, path_to_data)\n",
    "\n",
    "        ## Train\n",
    "        train_transform = SimCLRTransform(\n",
    "            input_size=input_size, vf_prob=0.5, rr_prob=0.5\n",
    "        )\n",
    "        train_dataset_simclr = LightlyDataset(\n",
    "            input_dir=path_to_train_data, transform=train_transform\n",
    "        )\n",
    "        dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "            train_dataset_simclr,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        mlflow.log_metric(\"train_dataset_length\", (len(train_dataset_simclr)))\n",
    "\n",
    "        ## Test\n",
    "        test_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize((input_size, input_size)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "                    std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        test_dataset_simclr = LightlyDataset(\n",
    "            input_dir=path_to_test_data, transform=test_transform\n",
    "        )\n",
    "        dataloader_test_simclr = torch.utils.data.DataLoader(\n",
    "            test_dataset_simclr,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        mlflow.log_metric(\"test_dataset_length\", (len(test_dataset_simclr)))\n",
    "\n",
    "        # Create model and callbacks\n",
    "        model = SimCLRModelVariableEmbeddingSize(\n",
    "            latent_space_size=latent_space_size,\n",
    "            projection_head_hidden_dim=projection_head_hidden_dim,\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"train_loss_ssl\", mode=\"min\", patience=5\n",
    "        )\n",
    "\n",
    "        model_checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"train_loss_ssl\", mode=\"min\", save_top_k=-1\n",
    "        )\n",
    "        loss_logging_callback = LossLoggingCallback()\n",
    "        hidden_dimensions_checking_callback = HiddenDimensionsCheckingCallback()\n",
    "\n",
    "        # Train the model\n",
    "        trainer = pl.Trainer(\n",
    "            log_every_n_steps=10,\n",
    "            max_epochs=max_epochs,\n",
    "            devices=1,\n",
    "            accelerator=accelerator,\n",
    "            callbacks=[\n",
    "                early_stopping_callback,\n",
    "                model_checkpoint_callback,\n",
    "                loss_logging_callback,\n",
    "                hidden_dimensions_checking_callback,\n",
    "            ],\n",
    "            # logger=mlflow_logger,\n",
    "            logger=csv_logger,\n",
    "        )\n",
    "        trainer.fit(model, dataloader_train_simclr)\n",
    "\n",
    "        # Generate embeddings\n",
    "        model.eval()\n",
    "        embeddings, filenames = generate_embeddings(model, dataloader_test_simclr)\n",
    "\n",
    "        # Visualize embeddings\n",
    "        pca = PCA(n_components=initial_num_components_pca)\n",
    "        data_pca = pca.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "        tsne = TSNE(n_components=2)\n",
    "        data_tsne = tsne.fit_transform(data_pca)\n",
    "\n",
    "        targets = [int(fname.split(\"\\\\\")[0]) for fname in filenames]  # windows\n",
    "        os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "        # Log embeddings params\n",
    "        mlflow.log_param(\n",
    "            \"actual_latent_space_size_sanity_check\", hidden_dimensions_checking_callback.latent_space_size\n",
    "        )\n",
    "        mlflow.log_param(\n",
    "            \"actual_projection_head_hidden_dim_sanity_check\",\n",
    "            hidden_dimensions_checking_callback.projection_head_hidden_dim,\n",
    "        )\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\n",
    "            \"final_train_loss_ssl\", loss_logging_callback.train_epoch_losses[-1]\n",
    "        )\n",
    "        mlflow.log_metric(\n",
    "            \"best_train_loss_ssl\", min(loss_logging_callback.train_epoch_losses)\n",
    "        )\n",
    "\n",
    "        # Save artifacts\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        explained_variance_path = f\"outputs/explained_variance_{classes_list}_{initial_num_components_pca}_{latent_space_size}.png\"\n",
    "        plt.bar(\n",
    "            range(1, len(explained_variance_ratio) + 1),\n",
    "            explained_variance_ratio,\n",
    "            alpha=0.4,\n",
    "            align=\"center\",\n",
    "        )\n",
    "        plt.step(\n",
    "            range(1, len(explained_variance_ratio) + 1),\n",
    "            np.cumsum(explained_variance_ratio),\n",
    "            where=\"mid\",\n",
    "        )\n",
    "        plt.xlabel(\"Principal Component Index\")\n",
    "        plt.ylabel(\"Explained Variance Ratio\")\n",
    "        plt.title(\"PCA Explained Variance Ratio\")\n",
    "        plt.savefig(explained_variance_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(explained_variance_path)\n",
    "\n",
    "        tsne_path = f\"outputs/tsne_{classes_list}_{initial_num_components_pca}_{latent_space_size}.png\"\n",
    "        plt.scatter(\n",
    "            data_tsne[:, 0], data_tsne[:, 1], c=targets, cmap=\"viridis\", alpha=0.4\n",
    "        )\n",
    "        plt.title(\"t-SNE visualization\")\n",
    "        plt.colorbar()\n",
    "        plt.savefig(tsne_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(tsne_path)\n",
    "\n",
    "        pca_path = f\"outputs/pca_{classes_list}_{initial_num_components_pca}_{latent_space_size}.png\"\n",
    "        plt.scatter(\n",
    "            data_pca[:, 0], data_pca[:, 1], c=targets, cmap=\"viridis\", alpha=0.4\n",
    "        )\n",
    "        plt.title(\"PCA visualization\")\n",
    "        plt.colorbar()\n",
    "        plt.savefig(pca_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(pca_path)\n",
    "\n",
    "        loss_plot_path = f\"outputs/loss_plot_{classes_list}_{initial_num_components_pca}_{latent_space_size}.png\"\n",
    "        plt.plot(\n",
    "            range(len(loss_logging_callback.train_epoch_losses)),\n",
    "            loss_logging_callback.train_epoch_losses,\n",
    "            marker=\"o\",\n",
    "        )\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss (Epoch)\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(loss_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
