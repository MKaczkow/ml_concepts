{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from: https://github.com/giakoumoglou/classification/blob/main/notebooks/main_simclr.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpXLJZYu1_4j"
   },
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2ERHkbgy1ulT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjomfTdt18o5"
   },
   "source": [
    "# 2. Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dt9H5Nf2vJ-"
   },
   "source": [
    "## 2.1 Metric Monitor\n",
    "Keeps track of average over values added to its instance. Useful to track accuracy over batches and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xnyphpqb10fG"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=4):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp1tsqfS2z5Z"
   },
   "source": [
    "## 2.2 Early Stopping\n",
    "Early stopping is a form of regularization used to avoid overfitting on the training dataset. Early stopping keeps track of the validation loss, if the loss stops decreasing for several epochs in a row the training stops. The ```EarlyStopping``` class is used to create an object to keep track of the validation loss. It will save a checkpoint of the model each time the validation loss decrease.  We set the ```patience``` argument to how many epochs we want to wait after the last time the validation loss improved before breaking the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zh7tkIkK13dQ"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Source:\n",
    "            https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'> early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLFtDthc2R8Z"
   },
   "source": [
    "## 2.3 Calculate Accuracy\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KZtARFoQ2hE_"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, target):\n",
    "    \"Calculates accuracy\"\n",
    "    output = output.data.max(dim=1,keepdim=True)[1]\n",
    "    output = output == 1.0\n",
    "    output = torch.flatten(output)\n",
    "    target = target == 1.0\n",
    "    target = torch.flatten(target)\n",
    "    return torch.true_divide((target == output).sum(dim=0), output.size(0)).item() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLtlGGP42-mk"
   },
   "source": [
    "## 2.4 Save Model\n",
    "Save the current state of the model alongside with the optimizer and the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aA92OLnc3AVr"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, save_file):\n",
    "    print('\\n==> Saving...')\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "    torch.save(state, save_file)\n",
    "    del state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htwdrWUS3GQu"
   },
   "source": [
    "## 2.5 Two Crop Transform\n",
    "Creates two crops of the same image which are the result of a stochastic augmentation defined as a transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8SaaWs9w3HqS"
   },
   "outputs": [],
   "source": [
    "class TwoCropTransform:\n",
    "    \"\"\"Create two crops of the same image\"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return [self.transform(x), self.transform(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13lsLXFQ2CtG"
   },
   "source": [
    "# 3. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrYrVHEv2L_d"
   },
   "source": [
    "## 3.1 Contrastive Framework SimCLR\n",
    "The contrastive framework SimCLR consists of:\n",
    "* Data augmentation module, ```Aug(·)```. For each input sample ```x```, we generate two random augmentations, each of which represents a different view of the data and contrains some subset of the information in the original sample\n",
    "* Encoder Network, ```Enc(·)```, which maps input sample ```x```' to a repesentation vector ```r=Enc(x)```. Both augmented samples are separately input to the same encoder, resulting in a pair of representation vectors. ```r``` is normalized to the unit hypersphere.\n",
    "* Projection Network, ```Proj(·)```, which maps ```r``` to a vector ```z=Proj(r)```. We instantiate as either a multi-layer perceptron with a single hidden layer of size 2048 and output vector of size 128 or just a single linear layer of size 128. We again normalize the output of this network to lie on the unit hypersphere, which enables using an inner product to measure distances in the projection space. As in self-supervised contrastive learning we discard ```Proj(·)``` at the end of contrastive training. As a result, our inference-time models contain exactly the same number of parameters as a cross-entropy model using the same encoder, ```Enc(·)```.\n",
    "* Contrastive loss functions defined for a contrastive prediction task. For the supervised version, see [Supervised Contrastive Learning](https://arxiv.org/pdf/2004.11362.pdf). For the self-supervised version, see [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VhCOJQS92eTr"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \"Encoder network\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # L1 (?, 28, 28, 1) -> (?, 28, 28, 32) -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L2 (?, 14, 14, 32) -> (?, 14, 14, 64) -> (?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        # L3 (?, 7, 7, 64) -> (?, 7, 7, 128) -> (?, 4, 4, 128)\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "            )\n",
    "        self._to_linear = 4 * 4 * 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten them for FC\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    \"\"\"Linear classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4 * 4 * 128, 10),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        probs = torch.nn.functional.softmax(x, dim=0)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupCon(nn.Module):\n",
    "    \"\"\"encoder + projection head\"\"\"\n",
    "    def __init__(self, model, head='mlp', feat_dim=128):\n",
    "        super(SupCon, self).__init__()\n",
    "        \n",
    "        self.dim_in = model._to_linear\n",
    "        self.encoder = model\n",
    "        \n",
    "        if head == 'linear':\n",
    "            self.head = nn.Linear(self.dim_in, feat_dim)\n",
    "        elif head == 'mlp':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(self.dim_in, self.dim_in),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(self.dim_in, feat_dim)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError('Head not supported: {}'.format(head))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        feat = F.normalize(self.head(feat), dim=1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Yonglong Tian (yonglong@mit.edu)\n",
    "Date: May 07, 2020\n",
    "\"\"\"\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRtcShmV2WvX"
   },
   "source": [
    "## 3.3 Pre-Training\n",
    "Main training loop of **encoder** and **projection head** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). Note that the loss used in the ```SupConLoss``` defined just for the pre-training. If CUDA is availiable, training will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VWJL-DyW2j5g"
   },
   "outputs": [],
   "source": [
    "def pretraining(epoch, model, contrastive_loader, optimizer, criterion, method='SimCLR'):\n",
    "    \"Contrastive pre-training over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    for batch_idx, (data,labels) in enumerate(contrastive_loader):\n",
    "        data = torch.cat([data[0], data[1]], dim=0)\n",
    "        if torch.cuda.is_available():\n",
    "            data,labels = data.cuda(), labels.cuda()\n",
    "        data, labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
    "        bsz = labels.shape[0]\n",
    "        features = model(data)\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        if method == 'SupCon':\n",
    "            loss = criterion(features, labels)\n",
    "        elif method == 'SimCLR':\n",
    "            loss = criterion(features)\n",
    "        else:\n",
    "            raise ValueError('contrastive method not supported: {}'.format(method))\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Learning Rate\", optimizer.param_groups[0]['lr'])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"[Epoch: {epoch:03d}] Contrastive Pre-train | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Learning Rate']['avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTEqEQpS2W_k"
   },
   "source": [
    "## 3.3 Training\n",
    "Main training loop of **classifier** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). Note that model, which is defined as the encoder and the projection head is set in evaluation mode and no training of its wieght happens. If CUDA is availiable, training will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nHCnAbSV2kj6"
   },
   "outputs": [],
   "source": [
    "def training(epoch, model, classifier, train_loader, optimizer, criterion):\n",
    "    \"Training over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    classifier.train()\n",
    "    for batch_idx, (data,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data,labels = data.cuda(), labels.cuda()\n",
    "        data, labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
    "        with torch.no_grad():\n",
    "            features = model.encoder(data)\n",
    "        output = classifier(features.float())\n",
    "        loss = criterion(output, labels) \n",
    "        accuracy = calculate_accuracy(output, labels)\n",
    "        metric_monitor.update(\"Loss\", loss.item())\n",
    "        metric_monitor.update(\"Accuracy\", accuracy)\n",
    "        data.detach()\n",
    "        labels.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"[Epoch: {epoch:03d}] Train      | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abiLjLhf3yQK"
   },
   "source": [
    "## 3.4 Validation\n",
    "Main validation loop of **encoder** and **classifier** for a number of batches over an epoch, as it is defined in [PyTorch](https://pytorch.org/). Projection head is not used for the validation process in SimCLR. If CUDA is availiable, training will take place in GPU. ```EarlyStopping``` class is used to keep track of loss and accuracy. Returns the loss and accuracy of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XF9NIV7T3zrR"
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, classifier, valid_loader, criterion):\n",
    "    \"Validation over an epoch\"\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data,labels) in enumerate(valid_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data,labels = data.cuda(), labels.cuda()\n",
    "            data, labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
    "            features = model.encoder(data)\n",
    "            output = classifier(features.float())\n",
    "            loss = criterion(output,labels) \n",
    "            accuracy = calculate_accuracy(output, labels)\n",
    "            metric_monitor.update(\"Loss\", loss.item())\n",
    "            metric_monitor.update(\"Accuracy\", accuracy)\n",
    "            data.detach()\n",
    "            labels.detach()\n",
    "    print(\"[Epoch: {epoch:03d}] Validation | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
    "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Accuracy']['avg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HUFJyf02aYV"
   },
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n5XDfwlD2loD"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    num_epochs = 100\n",
    "    use_early_stopping = True\n",
    "    use_scheduler = True\n",
    "    head_type = 'mlp' # choose among 'mlp' and 'linear'\n",
    "    save_file = os.path.join('./results/', 'model.pth')\n",
    "    if not os.path.isdir('./results/'):\n",
    "         os.makedirs('./results/')\n",
    "    \n",
    "    contrastive_transform = transforms.Compose([\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomResizedCrop(size=28, scale=(0.2, 1.)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                                       ])\n",
    "    train_transform = transforms.Compose([\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                                       ])\n",
    "    valid_transform = transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,), (0.5,)),\n",
    "                                       ])\n",
    "    \n",
    "    contrastive_set = datasets.MNIST('./data', download=True, train=True, transform=TwoCropTransform(contrastive_transform))\n",
    "    train_set = datasets.MNIST('./data', download=True, train=True, transform=train_transform)\n",
    "    valid_set = datasets.MNIST('./data', download=True, train=False, transform=valid_transform)\n",
    "    \n",
    "    contrastive_loader = torch.utils.data.DataLoader(contrastive_set, batch_size=64, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Part 1\n",
    "    encoder = Encoder()\n",
    "    model = SupCon(encoder, head=head_type, feat_dim=128)\n",
    "    criterion = SupConLoss(temperature=0.07)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "    contrastive_loss, contrastive_lr = [], []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss, lr = pretraining(epoch, model, contrastive_loader, optimizer, criterion, method='SimCLR')\n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "        contrastive_loss.append(loss)\n",
    "        contrastive_lr.append(lr)\n",
    "    \n",
    "    save_model(model, optimizer, num_epochs, save_file)\n",
    "    \n",
    "    plt.plot(range(1,len(contrastive_lr)+1),contrastive_lr, color='b', label = 'learning rate')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Learning Rate'), plt.show()\n",
    "    \n",
    "    plt.plot(range(1,len(contrastive_loss)+1),contrastive_loss, color='b', label = 'loss')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Loss'), plt.show()\n",
    "    \n",
    "    # Part 2\n",
    "    model = SupCon(encoder, head=head_type, feat_dim=128)\n",
    "    classifier = LinearClassifier()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    ckpt = torch.load(save_file, map_location='cpu')\n",
    "    state_dict = ckpt['model']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k = k.replace(\"module.\", \"\")\n",
    "        new_state_dict[k] = v\n",
    "    state_dict = new_state_dict\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    train_losses , train_accuracies = [],[]\n",
    "    valid_losses , valid_accuracies = [],[]\n",
    "    \n",
    "    if use_early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=30, verbose=False, delta=1e-4)\n",
    " \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        train_loss, train_accuracy = training(epoch, model, classifier, train_loader, optimizer, criterion)\n",
    "        valid_loss, valid_accuracy = validation(epoch, model, classifier, valid_loader, criterion)\n",
    "        \n",
    "        if use_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "             \n",
    "        if use_early_stopping: \n",
    "            early_stopping(valid_loss, model)\n",
    "            \n",
    "            if early_stopping.early_stop:\n",
    "                print('Early stopping at epoch', epoch)\n",
    "                #model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "                break\n",
    "     \n",
    "    plt.plot(range(1,len(train_losses)+1), train_losses, color='b', label = 'training loss')\n",
    "    plt.plot(range(1,len(valid_losses)+1), valid_losses, color='r', linestyle='dashed', label = 'validation loss')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Loss'), plt.show()\n",
    "     \n",
    "    plt.plot(range(1,len(train_accuracies)+1),train_accuracies, color='b', label = 'training accuracy')\n",
    "    plt.plot(range(1,len(valid_accuracies)+1),valid_accuracies, color='r', linestyle='dashed', label = 'validation accuracy')\n",
    "    plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Accuracy'), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PTxkyIgK2n2M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:06<00:00, 1416660.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 249350.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2271109.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4543412.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "[Epoch: 001] Contrastive Pre-train | Loss: 2.7749 | Learning Rate: 0.0010\n",
      "[Epoch: 002] Contrastive Pre-train | Loss: 1.7468 | Learning Rate: 0.0010\n",
      "[Epoch: 003] Contrastive Pre-train | Loss: 1.4683 | Learning Rate: 0.0010\n",
      "[Epoch: 004] Contrastive Pre-train | Loss: 1.3217 | Learning Rate: 0.0010\n",
      "[Epoch: 005] Contrastive Pre-train | Loss: 1.2261 | Learning Rate: 0.0010\n",
      "[Epoch: 006] Contrastive Pre-train | Loss: 1.1614 | Learning Rate: 0.0010\n",
      "[Epoch: 007] Contrastive Pre-train | Loss: 1.1127 | Learning Rate: 0.0010\n",
      "[Epoch: 008] Contrastive Pre-train | Loss: 1.0681 | Learning Rate: 0.0010\n",
      "[Epoch: 009] Contrastive Pre-train | Loss: 1.0274 | Learning Rate: 0.0010\n",
      "[Epoch: 010] Contrastive Pre-train | Loss: 1.0020 | Learning Rate: 0.0010\n",
      "[Epoch: 011] Contrastive Pre-train | Loss: 0.9788 | Learning Rate: 0.0010\n",
      "[Epoch: 012] Contrastive Pre-train | Loss: 0.9500 | Learning Rate: 0.0010\n",
      "[Epoch: 013] Contrastive Pre-train | Loss: 0.9452 | Learning Rate: 0.0010\n",
      "[Epoch: 014] Contrastive Pre-train | Loss: 0.9331 | Learning Rate: 0.0010\n",
      "[Epoch: 015] Contrastive Pre-train | Loss: 0.9173 | Learning Rate: 0.0010\n",
      "[Epoch: 016] Contrastive Pre-train | Loss: 0.9011 | Learning Rate: 0.0010\n",
      "[Epoch: 017] Contrastive Pre-train | Loss: 0.8773 | Learning Rate: 0.0010\n",
      "[Epoch: 018] Contrastive Pre-train | Loss: 0.8721 | Learning Rate: 0.0010\n",
      "[Epoch: 019] Contrastive Pre-train | Loss: 0.8674 | Learning Rate: 0.0010\n",
      "[Epoch: 020] Contrastive Pre-train | Loss: 0.8602 | Learning Rate: 0.0010\n",
      "[Epoch: 021] Contrastive Pre-train | Loss: 0.8506 | Learning Rate: 0.0009\n",
      "[Epoch: 022] Contrastive Pre-train | Loss: 0.8312 | Learning Rate: 0.0009\n",
      "[Epoch: 023] Contrastive Pre-train | Loss: 0.8307 | Learning Rate: 0.0009\n",
      "[Epoch: 024] Contrastive Pre-train | Loss: 0.8227 | Learning Rate: 0.0009\n",
      "[Epoch: 025] Contrastive Pre-train | Loss: 0.8218 | Learning Rate: 0.0009\n",
      "[Epoch: 026] Contrastive Pre-train | Loss: 0.8151 | Learning Rate: 0.0009\n",
      "[Epoch: 027] Contrastive Pre-train | Loss: 0.8179 | Learning Rate: 0.0009\n",
      "[Epoch: 028] Contrastive Pre-train | Loss: 0.7953 | Learning Rate: 0.0009\n",
      "[Epoch: 029] Contrastive Pre-train | Loss: 0.8087 | Learning Rate: 0.0009\n",
      "[Epoch: 030] Contrastive Pre-train | Loss: 0.7996 | Learning Rate: 0.0009\n",
      "[Epoch: 031] Contrastive Pre-train | Loss: 0.7837 | Learning Rate: 0.0009\n",
      "[Epoch: 032] Contrastive Pre-train | Loss: 0.7883 | Learning Rate: 0.0009\n",
      "[Epoch: 033] Contrastive Pre-train | Loss: 0.7896 | Learning Rate: 0.0009\n",
      "[Epoch: 034] Contrastive Pre-train | Loss: 0.7939 | Learning Rate: 0.0009\n",
      "[Epoch: 035] Contrastive Pre-train | Loss: 0.7917 | Learning Rate: 0.0009\n",
      "[Epoch: 036] Contrastive Pre-train | Loss: 0.7879 | Learning Rate: 0.0009\n",
      "[Epoch: 037] Contrastive Pre-train | Loss: 0.7763 | Learning Rate: 0.0009\n",
      "[Epoch: 038] Contrastive Pre-train | Loss: 0.7816 | Learning Rate: 0.0009\n",
      "[Epoch: 039] Contrastive Pre-train | Loss: 0.7885 | Learning Rate: 0.0009\n",
      "[Epoch: 040] Contrastive Pre-train | Loss: 0.7834 | Learning Rate: 0.0009\n",
      "[Epoch: 041] Contrastive Pre-train | Loss: 0.7551 | Learning Rate: 0.0008\n",
      "[Epoch: 042] Contrastive Pre-train | Loss: 0.7582 | Learning Rate: 0.0008\n",
      "[Epoch: 043] Contrastive Pre-train | Loss: 0.7459 | Learning Rate: 0.0008\n",
      "[Epoch: 044] Contrastive Pre-train | Loss: 0.7461 | Learning Rate: 0.0008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m contrastive_loss, contrastive_lr \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     loss, lr \u001b[38;5;241m=\u001b[39m \u001b[43mpretraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSimCLR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_scheduler:\n\u001b[0;32m     50\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mpretraining\u001b[1;34m(epoch, model, contrastive_loader, optimizer, criterion, method)\u001b[0m\n\u001b[0;32m      3\u001b[0m metric_monitor \u001b[38;5;241m=\u001b[39m MetricMonitor()\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data,labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(contrastive_loader):\n\u001b[0;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data[\u001b[38;5;241m0\u001b[39m], data[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m, in \u001b[0;36mTwoCropTransform.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(x)]\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:972\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     i, j, h, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresized_crop(img, i, j, h, w, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, antialias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:938\u001b[0m, in \u001b[0;36mRandomResizedCrop.get_params\u001b[1;34m(img, scale, ratio)\u001b[0m\n\u001b[0;32m    936\u001b[0m log_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(ratio))\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 938\u001b[0m     target_area \u001b[38;5;241m=\u001b[39m area \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m     aspect_ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39muniform_(log_ratio[\u001b[38;5;241m0\u001b[39m], log_ratio[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    941\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(target_area \u001b[38;5;241m*\u001b[39m aspect_ratio)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_classification_main_simclr.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
