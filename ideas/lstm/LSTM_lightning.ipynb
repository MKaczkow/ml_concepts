{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From:  \n",
    "https://www.kaggle.com/code/tartakovsky/pytorch-lightning-lstm-timeseries-clean-code/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InjXlZH1iIEH"
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BM37JXb_gyfB",
    "outputId": "c2969a70-b749-4c37-9abc-c43482ce9bf4"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"/data\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kbn5QPQgyfK"
   },
   "source": [
    "# Prediction task\n",
    "\n",
    "We are going to predict hourly levels of global active power one step ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu5mR7HfjKEJ"
   },
   "source": [
    "# TimeseriesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DiXnMThtgyfL"
   },
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset subclass.\n",
    "    Serves as input to DataLoader to transform X\n",
    "      into sequence data using rolling window.\n",
    "    DataLoader using this dataset will output batches\n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len - 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index : index + self.seq_len], self.y[index + self.seq_len - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxVFN7kaiNqR"
   },
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XM6DMN6-iM2-"
   },
   "outputs": [],
   "source": [
    "class PowerConsumptionDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading\n",
    "      and processing work in one place.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seq_len=1, batch_size=128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Data is resampled to hourly intervals.\n",
    "        Both 'np.nan' and '?' are converted to 'np.nan'\n",
    "        'Date' and 'Time' columns are merged into 'dt' index\n",
    "        \"\"\"\n",
    "\n",
    "        if stage == \"fit\" and self.X_train is not None:\n",
    "            return\n",
    "        if stage == \"test\" and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:\n",
    "            return\n",
    "\n",
    "        path = \"/__repos/ml_concepts/ideas/lstm/data/household_power_consumption.txt\"\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            sep=\";\",\n",
    "            parse_dates={\"dt\": [\"Date\", \"Time\"]},\n",
    "            infer_datetime_format=True,\n",
    "            low_memory=False,\n",
    "            na_values=[\"nan\", \"?\"],\n",
    "            index_col=\"dt\",\n",
    "        )\n",
    "\n",
    "        df_resample = df.resample(\"h\").mean()\n",
    "\n",
    "        X = df_resample.dropna().copy()\n",
    "        y = X[\"Global_active_power\"].shift(-1).ffill()\n",
    "        self.columns = X.columns\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(\n",
    "            self.X_train, self.y_train, seq_len=self.seq_len\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, self.y_val, seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, self.y_test, seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuTfMKIajx_C"
   },
   "source": [
    "# Model\n",
    "Implement LSTM regressor using pytorch-lighting module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-SkFq9n_gyfr"
   },
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        hidden_size,\n",
    "        seq_len,\n",
    "        batch_size,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        learning_rate,\n",
    "        criterion,\n",
    "    ):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:, -1])\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log(\"train_loss\", loss)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log(\"val_loss\", loss)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult()\n",
    "        result.log(\"test_loss\", loss)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbrpOYfc--uY"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yQukeupz_Blu"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "\"\"\"\n",
    "\n",
    "p = dict(\n",
    "    seq_len=24,\n",
    "    batch_size=70,\n",
    "    criterion=nn.MSELoss(),\n",
    "    max_epochs=10,\n",
    "    n_features=7,\n",
    "    hidden_size=100,\n",
    "    num_layers=1,\n",
    "    dropout=0.2,\n",
    "    learning_rate=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YvfIXTFkXTa"
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "evSq842Ygyfz",
    "outputId": "2cab8b21-a772-4d36-e082-565a4ef57463"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 43.6 K\n",
      "2 | linear    | Linear  | 101   \n",
      "--------------------------------------\n",
      "43.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.7 K    Total params\n",
      "0.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01299905776977539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dce4f4e8bd48dab656e99156ae521b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_lightning' has no attribute 'EvalResult'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\__repos\\ml_concepts\\ideas\\lstm\\LSTM_lightning.ipynb Komórka 15\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMRegressor(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     n_features \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mn_features\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     hidden_size \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     learning_rate \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m dm \u001b[39m=\u001b[39m PowerConsumptionDataModule(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     seq_len \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m p[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest(model, datamodule\u001b[39m=\u001b[39mdm)\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    698\u001b[0m )\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:737\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    734\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    735\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    736\u001b[0m )\n\u001b[1;32m--> 737\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    740\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1168\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1168\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1170\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1254\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1254\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1276\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1275\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[0;32m   1278\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m   1344\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m-> 1345\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1349\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[0;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    142\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[0;32m    231\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 240\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1706\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1706\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1708\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32md:\\__repos\\ml_concepts\\venv\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:370\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[0;32m    369\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32md:\\__repos\\ml_concepts\\ideas\\lstm\\LSTM_lightning.ipynb Komórka 15\u001b[0m in \u001b[0;36mLSTMRegressor.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(y_hat, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m result \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mEvalResult(checkpoint_on\u001b[39m=\u001b[39mloss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m result\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/__repos/ml_concepts/ideas/lstm/LSTM_lightning.ipynb#X16sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pytorch_lightning' has no attribute 'EvalResult'"
     ]
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "csv_logger = (CSVLogger(\"./\", name=\"lstm\", version=\"0\"),)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p[\"max_epochs\"],\n",
    "    logger=csv_logger,\n",
    "    # gpus=1,\n",
    "    # row_log_interval=1,\n",
    "    # progress_bar_refresh_rate=2,\n",
    ")\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features=p[\"n_features\"],\n",
    "    hidden_size=p[\"hidden_size\"],\n",
    "    seq_len=p[\"seq_len\"],\n",
    "    batch_size=p[\"batch_size\"],\n",
    "    criterion=p[\"criterion\"],\n",
    "    num_layers=p[\"num_layers\"],\n",
    "    dropout=p[\"dropout\"],\n",
    "    learning_rate=p[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "dm = PowerConsumptionDataModule(seq_len=p[\"seq_len\"], batch_size=p[\"batch_size\"])\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elE3vzV-yoI5"
   },
   "source": [
    "# Plot report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKAKWR3IlTk9",
    "outputId": "b06ed628-6564-482a-970b-319caa25cb1b"
   },
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(\"./lstm/0/metrics.csv\")\n",
    "train_loss = metrics[[\"train_loss\", \"step\", \"epoch\"]][~np.isnan(metrics[\"train_loss\"])]\n",
    "val_loss = metrics[[\"val_loss\", \"epoch\"]][~np.isnan(metrics[\"val_loss\"])]\n",
    "test_loss = metrics[\"test_loss\"].iloc[-1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "axes[0].set_title(\"Train loss per batch\")\n",
    "axes[0].plot(train_loss[\"step\"], train_loss[\"train_loss\"])\n",
    "axes[1].set_title(\"Validation loss per epoch\")\n",
    "axes[1].plot(val_loss[\"epoch\"], val_loss[\"val_loss\"], color=\"orange\")\n",
    "plt.show(block=True)\n",
    "\n",
    "print(\"MSE:\")\n",
    "print(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\n",
    "print(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\n",
    "print(f\"Test loss:  {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "82b3d9c33a00c8eb6e6d8c4c95ba207a84998d1fbb9c588cae47f7985e6a797d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
